{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Logistic regression learning algorithm using TensorFlow library.\n",
    "# Base example from Aymeric Damien, https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "\n",
    "# Currently a loose idea of what we need to make a model for \n",
    "# Still need to read in data\n",
    "# MNIST data kept in comments just in case we need to retest something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from tensorflow.python.platform import gfile\n",
    "#from tensorflow.examples.tutorials.mnist import input_data\n",
    "#mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read in data from chosen files\n",
    "data = []\n",
    "labels = []\n",
    "# correct user is specified for which labels will be marked as '1'\n",
    "correctUser = \"hannah\"\n",
    "path = './data'\n",
    "\n",
    "# read in data from files\n",
    "for filename in os.listdir(path):\n",
    "    fp = open(\"./data/\"+filename, \"r\")\n",
    "    j = json.load(fp)\n",
    "\n",
    "    # after file is open put the json data into lists for training\n",
    "    for user in j:\n",
    "        if user == correctUser:\n",
    "            for attempt in j[user]:\n",
    "                templist = []\n",
    "                for key in attempt[\"Keys\"]:\n",
    "                    templist.append(key[\"Fight\"])\n",
    "                    templist.append(key[\"Dwell\"])\n",
    "                templist.append(attempt[\"Metadata\"][0][\"Total Time\"])\n",
    "\n",
    "                data.append(templist)\n",
    "                labels.append(1)\n",
    "        else:\n",
    "            for attempt in j[user]:\n",
    "                templist = []\n",
    "                for key in attempt[\"Keys\"]:\n",
    "                    templist.append(key[\"Fight\"])\n",
    "                    templist.append(key[\"Dwell\"])\n",
    "                templist.append(attempt[\"Metadata\"][0][\"Total Time\"])\n",
    "\n",
    "                data.append(templist)\n",
    "                labels.append(0)\n",
    "\n",
    "data = np.asarray(data)\n",
    "labels = tf.one_hot(labels,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#normalize data input, feature scaling for better learning\n",
    "#theres probably a better way to do this but it works\n",
    "data = data.astype(float)\n",
    "for i in range(len(data)):\n",
    "    data[i] = (data[i] - np.mean(data[i])) / np.std(data[i])\n",
    "    \n",
    "#normThis = []\n",
    "#separate same values into separate lists to make math better\n",
    "#for j in range(len(data[0])):\n",
    "#    normThis.append([i[0] for i in data])\n",
    "    \n",
    "#print normThis[:2]\n",
    "#do the standardization\n",
    "#for i in range(len(normThis)):\n",
    "#    normThis[i] = (normThis[i] - np.mean(normThis[i])) / np.std(normThis[i])\n",
    "\n",
    "#print normThis[:2]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.2\n",
    "training_runs = 4000\n",
    "\n",
    "\n",
    "# tf Graph Input example from MNIST\n",
    "#x = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784\n",
    "#y = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition => 10 classes\n",
    "\n",
    "# tf Graph Input for PIN\n",
    "x = tf.placeholder(tf.float32, [None, 9]) # each entry has \n",
    "y = tf.placeholder(tf.float32, [None, 2]) # one hot vector should be the output\n",
    "\n",
    "# Set model weights\n",
    "W1 = tf.Variable(tf.random_normal([9, 2]))\n",
    "b1 = tf.Variable(tf.random_normal([2]))\n",
    "#W1 = tf.Variable(tf.random_normal([9, 20]))\n",
    "#b1 = tf.Variable(tf.random_normal([20]))\n",
    "#W2 = tf.Variable(tf.zeros([20, 2]))\n",
    "#b2 = tf.Variable(tf.zeros([2]))\n",
    "\n",
    "# Construct model\n",
    "pred = tf.nn.softmax(tf.matmul(x, W1) + b1)\n",
    "#z = tf.nn.softmax(tf.matmul(x, W1) + b1)\n",
    "#pred = tf.nn.softmax(tf.matmul(z, W2) + b2)\n",
    "\n",
    "# Minimize error using cross entropy\n",
    "#cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "cost = -tf.reduce_sum(y*tf.log(tf.clip_by_value(pred,1e-10,1.0)))\n",
    "\n",
    "# Gradient Descent\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[[ 0.9876008   0.0123992 ]\n",
      " [ 0.9906261   0.00937387]\n",
      " [ 0.99061781  0.00938217]\n",
      " [ 0.9908765   0.00912342]\n",
      " [ 0.99108022  0.00891973]\n",
      " [ 0.9873749   0.0126251 ]\n",
      " [ 0.99072582  0.00927427]\n",
      " [ 0.99016541  0.00983457]\n",
      " [ 0.99102497  0.00897501]\n",
      " [ 0.99106395  0.00893609]\n",
      " [ 0.98929977  0.01070019]\n",
      " [ 0.99022722  0.00977273]\n",
      " [ 0.99045092  0.00954907]\n",
      " [ 0.99042612  0.00957393]\n",
      " [ 0.99084407  0.00915587]]\n",
      "23.2584\n",
      "11.7536\n",
      "10.5326\n",
      "9.88293\n",
      "9.42507\n",
      "9.3422\n",
      "9.32071\n",
      "9.29141\n",
      "9.27024\n",
      "9.25555\n",
      "9.24026\n",
      "9.22441\n",
      "9.20806\n",
      "9.19123\n",
      "9.17393\n",
      "9.15619\n",
      "9.13804\n",
      "9.11953\n",
      "9.10069\n",
      "9.08155\n",
      "9.06215\n",
      "9.04251\n",
      "9.02267\n",
      "9.00265\n",
      "8.98248\n",
      "8.96218\n",
      "8.94178\n",
      "8.9213\n",
      "8.90075\n",
      "8.88017\n",
      "8.85956\n",
      "8.83894\n",
      "8.81832\n",
      "8.79774\n",
      "8.77718\n",
      "8.75668\n",
      "8.73623\n",
      "8.71586\n",
      "8.69556\n",
      "8.67536\n",
      "8.65525\n",
      "8.63524\n",
      "8.61534\n",
      "8.59556\n",
      "8.57589\n",
      "8.55634\n",
      "8.53692\n",
      "8.51763\n",
      "8.49846\n",
      "8.47943\n",
      "8.46052\n",
      "8.44175\n",
      "8.4231\n",
      "8.40459\n",
      "8.3862\n",
      "8.36794\n",
      "8.34981\n",
      "8.33179\n",
      "8.3139\n",
      "8.29613\n",
      "8.27847\n",
      "8.26093\n",
      "8.24349\n",
      "8.22616\n",
      "8.20894\n",
      "8.19181\n",
      "8.17478\n",
      "8.15784\n",
      "8.14099\n",
      "8.12423\n",
      "8.10754\n",
      "8.09094\n",
      "8.07441\n",
      "8.05795\n",
      "8.04156\n",
      "8.02523\n",
      "8.00896\n",
      "7.99275\n",
      "7.9766\n",
      "7.96049\n",
      "7.94443\n",
      "7.92842\n",
      "7.91245\n",
      "7.89652\n",
      "7.88062\n",
      "7.86475\n",
      "7.84892\n",
      "7.83311\n",
      "7.81733\n",
      "7.80158\n",
      "7.78584\n",
      "7.77012\n",
      "7.75442\n",
      "7.73873\n",
      "7.72306\n",
      "7.7074\n",
      "7.69174\n",
      "7.6761\n",
      "7.66046\n",
      "7.64483\n",
      "7.6292\n",
      "7.61357\n",
      "7.59794\n",
      "7.58231\n",
      "7.56668\n",
      "7.55105\n",
      "7.53542\n",
      "7.51978\n",
      "7.50414\n",
      "7.48848\n",
      "7.47283\n",
      "7.45717\n",
      "7.4415\n",
      "7.42582\n",
      "7.41014\n",
      "7.39445\n",
      "7.37874\n",
      "7.36303\n",
      "7.34731\n",
      "7.33158\n",
      "7.31584\n",
      "7.30009\n",
      "7.28433\n",
      "7.26856\n",
      "7.25278\n",
      "7.23699\n",
      "7.22119\n",
      "7.20538\n",
      "7.18956\n",
      "7.17374\n",
      "7.1579\n",
      "7.14206\n",
      "7.1262\n",
      "7.11034\n",
      "7.09447\n",
      "7.07859\n",
      "7.06271\n",
      "7.04681\n",
      "7.03091\n",
      "7.01501\n",
      "6.99909\n",
      "6.98318\n",
      "6.96725\n",
      "6.95133\n",
      "6.93539\n",
      "6.91946\n",
      "6.90352\n",
      "6.88758\n",
      "6.87163\n",
      "6.85568\n",
      "6.83974\n",
      "6.82379\n",
      "6.80793\n",
      "6.82562\n",
      "6.7796\n",
      "6.83696\n",
      "6.7662\n",
      "6.73413\n",
      "6.72373\n",
      "6.70573\n",
      "6.69205\n",
      "6.67747\n",
      "6.66345\n",
      "6.64936\n",
      "6.63525\n",
      "6.62116\n",
      "6.60706\n",
      "6.59297\n",
      "6.57886\n",
      "6.56476\n",
      "6.55065\n",
      "6.53653\n",
      "6.52242\n",
      "6.5083\n",
      "6.4942\n",
      "6.48272\n",
      "6.85205\n",
      "6.5193\n",
      "6.4638\n",
      "6.43253\n",
      "6.41868\n",
      "6.40291\n",
      "6.38937\n",
      "6.37686\n",
      "6.36399\n",
      "6.35125\n",
      "6.33856\n",
      "6.32587\n",
      "6.31318\n",
      "6.3005\n",
      "6.2878\n",
      "6.27511\n",
      "6.2624\n",
      "6.24971\n",
      "6.23713\n",
      "6.23388\n",
      "6.41664\n",
      "6.24877\n",
      "6.18811\n",
      "6.17978\n",
      "6.16763\n",
      "6.1539\n",
      "6.14131\n",
      "6.12924\n",
      "6.11733\n",
      "6.10553\n",
      "6.09379\n",
      "6.08201\n",
      "6.07024\n",
      "6.05849\n",
      "6.04734\n",
      "6.0712\n",
      "6.05848\n",
      "6.013\n",
      "6.00406\n",
      "5.9938\n",
      "5.98108\n",
      "5.96887\n",
      "5.95735\n",
      "5.94624\n",
      "5.93531\n",
      "5.92425\n",
      "5.91328\n",
      "5.90233\n",
      "5.89215\n",
      "5.90738\n",
      "5.9541\n",
      "5.8891\n",
      "5.85952\n",
      "5.84267\n",
      "5.82983\n",
      "5.81774\n",
      "5.80674\n",
      "5.79652\n",
      "5.78599\n",
      "5.77558\n",
      "5.76579\n",
      "5.77269\n",
      "5.8694\n",
      "5.77661\n",
      "5.74072\n",
      "5.72014\n",
      "5.7059\n",
      "5.69463\n",
      "5.68511\n",
      "5.67485\n",
      "5.66509\n",
      "5.65554\n",
      "5.65061\n",
      "5.72363\n",
      "5.62592\n",
      "5.61841\n",
      "5.61121\n",
      "5.60018\n",
      "5.58769\n",
      "5.57873\n",
      "5.56866\n",
      "5.55911\n",
      "5.54964\n",
      "5.54082\n",
      "5.56675\n",
      "5.56834\n",
      "5.51612\n",
      "5.50386\n",
      "5.49611\n",
      "5.48648\n",
      "5.47687\n",
      "5.46766\n",
      "5.45877\n",
      "5.44989\n",
      "5.4409\n",
      "5.43199\n",
      "5.42309\n",
      "5.41446\n",
      "5.41302\n",
      "5.53645\n",
      "5.4277\n",
      "5.39424\n",
      "5.37633\n",
      "5.36362\n",
      "5.35374\n",
      "5.34533\n",
      "5.33691\n",
      "5.32824\n",
      "5.31973\n",
      "5.31128\n",
      "5.3033\n",
      "5.31055\n",
      "5.42596\n",
      "5.32307\n",
      "5.28643\n",
      "5.26764\n",
      "5.25588\n",
      "5.24603\n",
      "5.23726\n",
      "5.22933\n",
      "5.22111\n",
      "5.21309\n",
      "5.20516\n",
      "5.19907\n",
      "5.23589\n",
      "5.20149\n",
      "5.18774\n",
      "5.1731\n",
      "5.16166\n",
      "5.15091\n",
      "5.1421\n",
      "5.13469\n",
      "5.1267\n",
      "5.11889\n",
      "5.11116\n",
      "5.10369\n",
      "5.10824\n",
      "5.26991\n",
      "5.1328\n",
      "5.08465\n",
      "5.06714\n",
      "5.05865\n",
      "5.05129\n",
      "5.04404\n",
      "5.03677\n",
      "5.02938\n",
      "5.02202\n",
      "5.0147\n",
      "5.0074\n",
      "5.00015\n",
      "4.99421\n",
      "5.02775\n",
      "5.00644\n",
      "4.98221\n",
      "4.96765\n",
      "4.95894\n",
      "4.95143\n",
      "4.94386\n",
      "4.93621\n",
      "4.92927\n",
      "4.92218\n",
      "4.91521\n",
      "4.9084\n",
      "4.90455\n",
      "4.9757\n",
      "4.8873\n",
      "4.88077\n",
      "4.87421\n",
      "4.86795\n",
      "4.86153\n",
      "4.85425\n",
      "4.84706\n",
      "4.84047\n",
      "4.83376\n",
      "4.8272\n",
      "4.82299\n",
      "4.86941\n",
      "4.81828\n",
      "4.81053\n",
      "4.80045\n",
      "4.79108\n",
      "4.782\n",
      "4.77462\n",
      "4.76845\n",
      "4.7618\n",
      "4.75528\n",
      "4.74895\n",
      "4.74531\n",
      "4.81765\n",
      "4.73037\n",
      "4.72416\n",
      "4.7178\n",
      "4.71109\n",
      "4.70484\n",
      "4.69893\n",
      "4.69271\n",
      "4.68632\n",
      "4.6802\n",
      "4.67403\n",
      "4.66805\n",
      "4.6651\n",
      "4.73124\n",
      "4.64956\n",
      "4.64449\n",
      "4.63872\n",
      "4.63309\n",
      "4.62685\n",
      "4.61986\n",
      "4.61377\n",
      "4.60777\n",
      "4.60191\n",
      "4.59634\n",
      "4.59634\n",
      "4.68118\n",
      "4.58284\n",
      "4.57234\n",
      "4.56741\n",
      "4.56266\n",
      "4.55615\n",
      "4.54941\n",
      "4.54392\n",
      "4.53812\n",
      "4.53258\n",
      "4.53008\n",
      "All done!\n",
      "predictions: [0 0 0 0 0 0 0 0 0 0 1 1 1 1 0]\n",
      "[[ 0.95625418  0.04374584]\n",
      " [ 0.94412625  0.05587378]\n",
      " [ 0.68498629  0.31501365]\n",
      " [ 0.61065614  0.38934383]\n",
      " [ 0.63255924  0.36744073]\n",
      " [ 0.89648795  0.103512  ]\n",
      " [ 0.97435457  0.02564542]\n",
      " [ 0.77069759  0.22930239]\n",
      " [ 0.79910129  0.20089875]\n",
      " [ 0.61198199  0.38801798]\n",
      " [ 0.17799106  0.82200897]\n",
      " [ 0.32113659  0.67886341]\n",
      " [ 0.39077026  0.6092298 ]\n",
      " [ 0.13199575  0.8680042 ]\n",
      " [ 0.55853015  0.44146988]]\n",
      "actual:  [[ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "sess=tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "#turn labels back into an np array, needed session to be started before running\n",
    "l = sess.run(labels)\n",
    "\n",
    "#Create a saver object which will save all the variables\n",
    "#saved_result= tf.Variable(data, name=\"saved_result\")\n",
    "#do_save=tf.assign(saved_result,pred)\n",
    "#tf.train.write_graph(sess.graph_def, \"/tmp/load\", \"test.pb\", False) #proto\n",
    "\n",
    "# Training cycle\n",
    "print \"predictions:\", tf.argmax(pred.eval(feed_dict={x:data[:15]}),1).eval()\n",
    "print pred.eval(feed_dict={x:data[:15]})\n",
    "for i in range(training_runs):\n",
    "    # Fit training using json data\n",
    "    _, error = sess.run([optimizer, cost], {x:data[:15], y:l[:15]})\n",
    "\n",
    "    # Print error every once and a while to see whats going on\n",
    "    if (i % 10) == 0:\n",
    "        #print sess.run(W1)\n",
    "        #print sess.run(b1)\n",
    "        print error\n",
    "\n",
    "print \"All done!\"\n",
    "print \"predictions:\", tf.argmax(pred.eval(feed_dict={x:data[:15]}),1).eval()\n",
    "print pred.eval(feed_dict={x:data[:15]})\n",
    "print \"actual: \", l[:15]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test.data'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now set the data:\n",
    "saver = tf.train.Saver()\n",
    "tf.train.write_graph(sess.graph_def, \".\", \"test.pb\", False) #proto\n",
    "saver.save(sess,\"test.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "y: [[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (5, 784) for Tensor u'Placeholder_16:0', which has shape '(?, 9)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-f25090cac8b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;31m# Fit training using batch data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs,\n\u001b[0;32m---> 17\u001b[0;31m                                                           y: batch_ys})\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0;31m# Compute average loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mavg_cost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    941\u001b[0m                 \u001b[0;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m                 \u001b[0;34m'which has shape %r'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m    944\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (5, 784) for Tensor u'Placeholder_16:0', which has shape '(?, 9)'"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "# not using, this is just a reference\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(10):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/200)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(200)\n",
    "            # Fit training using batch data\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs,\n",
    "                                                          y: batch_ys})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost)\n",
    "            print sess.run(b1)\n",
    "\n",
    "    print \"Optimization Finished!\"\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy for 3000 examples\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print \"Accuracy:\", accuracy.eval({x: mnist.test.images[:3000], y: mnist.test.labels[:3000]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
